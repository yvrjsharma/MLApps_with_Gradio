{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPZIL/obRQzvdjQWmYb68cT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yvrjsharma/MLApps_with_Gradio/blob/main/Pix2PixZero_Demo_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install Pillow==9.0.0 --q\n",
        "!pip install --ignore-installed Pillow==9.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "Pl_TZh1IVyx3",
        "outputId": "a996c516-ad5b-422c-8c11-130fa2056d0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pillow==9.0.0\n",
            "  Using cached Pillow-9.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "Installing collected packages: Pillow\n",
            "Successfully installed Pillow-9.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate diffusers --q"
      ],
      "metadata": {
        "id": "TSyVkiwAU-Dp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install salesforce-lavis --q"
      ],
      "metadata": {
        "id": "cRTj_AOGUHAO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IiXjLopTUJS",
        "outputId": "18226d3a-5ea4-4f4e-f27c-c1c01669841e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pix2pix-zero' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/pix2pixzero/pix2pix-zero.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/pix2pixzero/pix2pix-zero.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGfzrAjfThDR",
        "outputId": "c731e21d-cf93-4427-b28a-c3bc20378081"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pix2pixzero/pix2pix-zero.git\n",
            "  Cloning https://github.com/pix2pixzero/pix2pix-zero.git to /tmp/pip-req-build-zmosz2di\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pix2pixzero/pix2pix-zero.git /tmp/pip-req-build-zmosz2di\n",
            "  Resolved https://github.com/pix2pixzero/pix2pix-zero.git to commit 25b885767bf81fc8bd9e7aa7a2a095ecbebab9db\n",
            "\u001b[31mERROR: git+https://github.com/pix2pixzero/pix2pix-zero.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pix2pix-zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFhnJeO6T4mm",
        "outputId": "fbb0cf70-0e91-45b6-fbfe-e47a311e16e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pix2pix-zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYxiwMgyUACK",
        "outputId": "c9f0eb66-bff7-4fce-a4de-c661add51bbe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pix2pix-zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, running the inversion command to obtain the input noise that will  reconstruct the image. \n",
        "# It will save the inversion as output/test_cat/inversion/cat_1.pt \n",
        "# and BLIP-generated prompt as output/test_cat/prompt/cat_1.txt - a painting of a cat sitting on top of a ball\n",
        "! python src/inversion.py  \\\n",
        "        --input_image \"assets/test_images/cats/cat_1.png\" \\\n",
        "        --results_folder \"output/test_cat\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqRHqtVvThGM",
        "outputId": "0d8a419f-6c3e-4d0d-f957-aca6eaa1fcb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 16:42:59.443740: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 16:42:59.645586: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 16:43:00.474182: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-11 16:43:00.474285: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-11 16:43:00.474306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 138654.68it/s]\n",
            " 96% 48/50 [00:10<00:00,  4.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, perform image editing with the editing direction as shown. \n",
        "# The command will save the edited image as output/test_cat/edit/cat_1.png\n",
        "! python src/edit_real.py \\\n",
        "    --inversion \"output/test_cat/inversion/cat_1.pt\" \\\n",
        "    --prompt \"output/test_cat/prompt/cat_1.txt\" \\\n",
        "    --task_name \"cat2dog\" \\\n",
        "    --results_folder \"output/test_cat/\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H_qsmKNTgXb",
        "outputId": "659d4ecd-a12d-4c25-a27c-da4b140bc8a2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 19:13:52.857177: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 19:13:53.012536: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 19:13:53.784729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 19:13:53.784824: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 19:13:53.784844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 182857.94it/s]\n",
            "100% 50/50 [00:09<00:00,  5.02it/s]\n",
            "100% 50/50 [00:19<00:00,  2.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from PIL import Image\n",
        "\n",
        "# First, running the inversion command to obtain the input noise that will  reconstruct the image. \n",
        "# It will save the inversion as output/test_cat/inversion/cat_1.pt \n",
        "# and BLIP-generated prompt as output/test_cat/prompt/cat_1.txt - a painting of a cat sitting on top of a ball\n",
        "def inversion(image_in, progress=gr.Progress(track_tqdm=True)):\n",
        "  progress(0, desc=\"Starting...\")\n",
        "  #img = Image.open(image_in)\n",
        "  image_in.save(\"assets/test_images/cats/input_image.png\")\n",
        "  # Run the script.py file\n",
        "  subprocess.run([\"python\", \"src/inversion.py\", \"--input_image\", \"input_image.png\", \"--results_folder\", \"output/test_cat\"])\n",
        "  # Open the text file with blip caption\n",
        "  with open(\"output/test_cat/prompt/input_image.txt\", \"r\") as file:\n",
        "      # Read the file line by line\n",
        "      prompt = file.read()\n",
        "  return \"output/test_cat/inversion/input_image.pt\", prompt\n",
        "\n",
        "\n",
        "# Next, perform image editing with the editing direction as shown. \n",
        "# The command will save the edited image as output/test_cat/edit/cat_1.png\n",
        "def image_edit(task_name, progress=gr.Progress(track_tqdm=True)):\n",
        "  progress(0, desc=\"Starting...\")\n",
        "  # Run the script.py file\n",
        "  subprocess.run([\"python\", \"src/edit_real.py\", \"--inversion\", \"output/test_cat/inversion/input_image.pt\", \n",
        "                  \"--prompt\", \"output/test_cat/prompt/input_image.txt\", \"--task_name\", task_name,\n",
        "                  \"--results_folder\", \"output/test_cat/\"])\n",
        "  return \"output/test_cat/edit/input_image.png\"\n",
        "  \n",
        "\n",
        "#Similarly, we can edit the synthetic images generated by Stable Diffusion with the following command.\n",
        "def synthetic_image_edit(prompt, task_name, progress=gr.Progress(track_tqdm=True)):\n",
        "  progress(0, desc=\"Starting...\")\n",
        "  # Run the script file\n",
        "  subprocess.run([\"python\", \"src/edit_synthetic.py\", \"--prompt_str\", prompt, \n",
        "                  \"--task\", task_name, \"--results_folder\", \"output/synth_editing\"])\n",
        "  return \"output/synth_editing/reconstruction.png\", \"output/synth_editing/edit.png\"\n",
        "  \n",
        "\n",
        "\n",
        "#def initial_radio_config(gr.Radio()):\n",
        "#    if gr.Radio() == \"Edit Synthetic (SD) Images\":\n",
        "#        return gr.update(visible=True), gr.update(visible=True)\n",
        "    \n",
        "\n",
        "#def button_visiblity():\n",
        "#    return gr.update(visible=False)\n",
        "\n",
        "def set_visible_true():\n",
        "    return gr.update(visible=True)\n",
        "\n",
        "def set_visible_False():\n",
        "    return gr.update(visible=False)"
      ],
      "metadata": {
        "id": "xL0UTQqNDvmw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  gr.HTML(\"\"\"<div style=\"text-align: center; max-width: 700px; margin: 0 auto;\">\n",
        "                  <div\n",
        "              style=\"\n",
        "                  display: inline-flex;\n",
        "                  align-items: center;\n",
        "                  gap: 0.8rem;\n",
        "                  font-size: 1.75rem;\n",
        "              \"\n",
        "              >\n",
        "              <h1 style=\"font-weight: 900; margin-bottom: 7px; margin-top: 5px;\">\n",
        "                  Pix2Pix - Zero-shot Image-to-Image Translation\n",
        "              </h1>\n",
        "              </div>\n",
        "              <p style=\"margin-bottom: 10px; font-size: 94%\">\n",
        "              This is an unofficial demo for <a href=\"https://pix2pixzero.github.io/\" target=\"_blank\">Pix2PixZero</a>. \n",
        "              Please visit their website and <a href=\"https://github.com/pix2pixzero/pix2pix-zero\" target=\"_blank\">github repo</a> for more details.\n",
        "              </p></div>\"\"\")\n",
        "  #task_initial_radio  = gr.Radio(choices = [\"Image Translation\", \"Edit Synthetic (SD) Images\",], type=\"value\", visible=True, label=\"Select a task that you want to perform using Pix2Pix-Zero\") \n",
        "  with gr.Row():\n",
        "    image_in = gr.Image(type=\"pil\", label=\"Upload the image of a cat or a dog you want to translate\")\n",
        "    with gr.Column():\n",
        "      btn_inversion = gr.Button(\"Get input noise\",visible=False )\n",
        "      with gr.Row():\n",
        "        blip_prompt = gr.Textbox(visible=False)\n",
        "        inversion_file = gr.File(visible=False)\n",
        "        #task_name = gr.Textbox()\n",
        "  with gr.Row():\n",
        "    image_out = gr.Image(visible=False, label=\"Trnaslated Image output\")\n",
        "    with gr.Column():\n",
        "      task_name_radio  = gr.Radio(choices = [\"cat2dog\", \"dog2cat\",], type=\"value\", visible=False, label=\"Select a task that you want to accomplish\") #, value=\"cat2dog\"),\n",
        "      btn_imageedit = gr.Button(value=\"Edit the image!\",visible=False)\n",
        "      prompt_synth = gr.Textbox(visible=False)\n",
        "      btn_synth_image = gr.Button(value=\"Generate & Translate SD image\",visible=False)\n",
        "  with gr.Row():\n",
        "    image_synth = gr.Image(visible=False, label=\"Synthetic image generated by Stable Diffusion on the fly\")\n",
        "    image_synth_translated = gr.Image(visible=False, label=\"Translated synthetic image\")\n",
        "\n",
        "    btn_inversion.click(inversion,[image_in],[inversion_file, blip_prompt])\n",
        "    btn_inversion.click(set_visible_true, [], task_name_radio)  #inversion_file, blip_prompt,\n",
        "    btn_inversion.click(set_visible_False, [], btn_inversion)\n",
        "\n",
        "    #task_initial_radio.change(set_visible_true, [], btn_imageedit)\n",
        "    \n",
        "    task_name_radio.change(set_visible_true, [], btn_imageedit)\n",
        "    task_name_radio.change(set_visible_true, [], image_out)\n",
        "\n",
        "    btn_imageedit.click(image_edit,[task_name_radio],[image_out])\n",
        "    btn_imageedit.click(set_visible_False, [], btn_imageedit)\n",
        "    btn_imageedit.click(set_visible_true, [], prompt_synth)\n",
        "    btn_imageedit.click(set_visible_true, [], btn_synth_image)\n",
        "    btn_imageedit.click(set_visible_true, [], image_synth)\n",
        "    btn_imageedit.click(set_visible_true, [], image_synth_translated)\n",
        "    btn_synth_image.click(synthetic_image_edit,[prompt_synth, task_name_radio],[image_synth, image_synth_translated])\n",
        "\n",
        "    image_in.clear(set_visible_true, [], btn_inversion)\n",
        "    image_in.change(set_visible_true, [], btn_inversion)\n",
        "    image_in.change(set_visible_true, [], blip_prompt)\n",
        "    image_in.change(set_visible_true, [], inversion_file)\n",
        "\n",
        "demo.queue()\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "usW8BH5LCm_t",
        "outputId": "3dee9570-8f7b-4b84-c751-137cb06c2122"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f23e6ea4-0ec1-4ebd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f23e6ea4-0ec1-4ebd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f23e6ea4-0ec1-4ebd.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inversion - my own\n",
        "Testing for other images then cat and dog\n",
        "- Results only available tasks are - cat2dog and dog2cat"
      ],
      "metadata": {
        "id": "leySd-D-fh-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inversion\n",
        "! python src/inversion.py  \\\n",
        "        --input_image \"assets/test_images/cats/shiba1.jpg\" \\\n",
        "        --results_folder \"output/test_cat\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7cTs5JuX6IA",
        "outputId": "d6171046-a4d3-4dde-d0d7-0c4ac4d37dbb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 17:24:19.079208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 17:24:19.281621: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 17:24:20.106808: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-11 17:24:20.106911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2023-02-11 17:24:20.106930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 189039.05it/s]\n",
            " 96% 48/50 [00:10<00:00,  4.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#image editing with directions\n",
        "# blip desc - a brown and white dog standing in front of a white background\n",
        "! python src/edit_real.py \\\n",
        "    --inversion \"output/test_cat/inversion/shiba1.pt\" \\\n",
        "    --prompt \"output/test_cat/prompt/cat_1.txt\" \\\n",
        "    --task_name \"dog2cat\" \\\n",
        "    --results_folder \"output/test_cat/\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiEEPjhRfqDc",
        "outputId": "d0748e82-704f-490f-9d1a-541cc3eaaac4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 17:26:21.216701: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 17:26:21.372151: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 17:26:22.152430: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 17:26:22.152521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 17:26:22.152540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 96978.13it/s]\n",
            "100% 50/50 [00:09<00:00,  5.01it/s]\n",
            "100% 50/50 [00:19<00:00,  2.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "Fyk4nEVTCKq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QlaWeas5Cmlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Editing images on the fly while generating through SD"
      ],
      "metadata": {
        "id": "QYNabYCljURQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "! python src/edit_synthetic.py \\\n",
        "    --results_folder \"output/synth_editing\" \\\n",
        "    --prompt_str \"a high resolution painting of a cat in the style of van gough\" \\\n",
        "    --task \"cat2dog\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7QDBGX6fp77",
        "outputId": "fdf2e378-d89e-482e-d02c-2741434a5456"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 17:37:22.871869: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 17:37:23.031386: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 195652.66it/s]\n",
            "100% 50/50 [00:09<00:00,  5.02it/s]\n",
            "100% 50/50 [00:19<00:00,  2.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom edit instructions"
      ],
      "metadata": {
        "id": "48gaAiihlQo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python src/make_edit_direction.py \\\n",
        "    --file_source_sentences sentences/apple.txt \\\n",
        "    --file_target_sentences sentences/orange.txt \\\n",
        "    --output_folder assets/embeddings_sd_1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5lOBTEmfp3f",
        "outputId": "4cbb6ae3-50a1-4a01-cfe1-464259f6dc6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 17:48:15.759994: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 17:48:15.914254: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 17:48:16.666646: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 17:48:16.666751: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 17:48:16.666773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 112977.89it/s]\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/pix2pix-zero/src/\u001b[0m\u001b[1;33mmake_edit_direction.py\u001b[0m:\u001b[94m46\u001b[0m in \u001b[92m<module>\u001b[0m              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m43 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m os.path.exists(outf_src):                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m44 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mSkipping source file \u001b[0m\u001b[33m{\u001b[0moutf_src\u001b[33m}\u001b[0m\u001b[33m as it already exists\u001b[0m\u001b[33m\"\u001b[0m)  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m46 \u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(args.file_source_sentences, \u001b[33m\"\u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mas\u001b[0m f:                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   │   │   \u001b[0ml_sents = [x.strip() \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m f.readlines()]                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m│   │   \u001b[0mmean_emb = load_sentence_embeddings(l_sents, pipe.tokenizer, pi \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(mean_emb.shape)                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mFileNotFoundError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \u001b[32m'sentences/apple.txt'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying something else now  -"
      ],
      "metadata": {
        "id": "b5Jl5KJDoXkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, running the inversion command to obtain the input noise that will  reconstruct the image. \n",
        "# It will save the inversion as output/test_cat/inversion/cat_1.pt \n",
        "# and BLIP-generated prompt as output/test_cat/prompt/cat_1.txt - a painting of a cat sitting on top of a ball\n",
        "! python src/inversion.py  \\\n",
        "        --input_image \"assets/test_images/cats/cat_1.png\" \\\n",
        "        --results_folder \"output/test_cat\""
      ],
      "metadata": {
        "id": "qXyuiJe4fp1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, perform image editing with the editing direction as shown. \n",
        "# The command will save the edited image as output/test_cat/edit/cat_1.png\n",
        "! python src/edit_real.py \\\n",
        "    --inversion \"output/test_cat/inversion/cat_1.pt\" \\\n",
        "    --prompt \"output/test_cat/prompt/cat_2.txt\" \\\n",
        "    --results_folder \"output/test_cat/\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05_aUQFIfpx_",
        "outputId": "140ced22-5926-43cc-a69e-3229a93f9f18"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 18:08:03.385624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 18:08:03.542149: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 18:08:04.305956: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 18:08:04.306050: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 18:08:04.306071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 46345.90it/s]\n",
            "100% 50/50 [00:09<00:00,  5.02it/s]\n",
            "100% 50/50 [00:19<00:00,  2.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "! python src/edit_synthetic.py \\\n",
        "    --results_folder \"output/synth_editing\" \\\n",
        "    --prompt_str \"a painting of a tiger sitting on top of a ball\" \\\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOADs1aHfpvu",
        "outputId": "83958ab6-fc1d-4ed1-c3e1-9b43e4412563"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-11 18:15:20.524434: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 18:15:20.679830: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 18:15:21.442744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 18:15:21.442836: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-11 18:15:21.442853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Fetching 16 files: 100% 16/16 [00:00<00:00, 26327.53it/s]\n",
            "100% 50/50 [00:09<00:00,  5.02it/s]\n",
            "100% 50/50 [00:19<00:00,  2.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSZUG6m6fptc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}